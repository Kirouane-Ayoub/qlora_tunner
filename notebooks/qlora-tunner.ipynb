{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_15n15-NfU20"
      },
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gri0a3JCSiz9"
      },
      "outputs": [],
      "source": [
        "!pip -q  install qlora-tunner==1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pyIgtSuY_XIV"
      },
      "outputs": [],
      "source": [
        "from qlora_tunner.qlora_fine_tuner import LanguageModelFineTuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xOvRnbwk_ft8"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "fine_tuner = LanguageModelFineTuner(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bntNOCWfjeTg"
      },
      "source": [
        "## Load Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tv2WGpuWAhOS"
      },
      "outputs": [],
      "source": [
        "from qlora_tunner.utils import data_reformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDYlzNyKjDvO"
      },
      "outputs": [],
      "source": [
        "system_message = \"\" \n",
        "train_dataset_path = \"\" # train.jsonl\n",
        "valid_dataset_path = \"\" # valid.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya_2_2o-ArTI"
      },
      "outputs": [],
      "source": [
        "train_dataset_mapped = data_reformer(inp_ut=\"prompt\" ,\n",
        "                                     output=\"response\" ,\n",
        "                                     dataset_path=train_dataset_path,\n",
        "                                     system_message=system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "764v0n8Xfvc3"
      },
      "outputs": [],
      "source": [
        "valid_dataset_mapped = data_reformer(inp_ut=\"prompt\" ,\n",
        "                                     output=\"response\" ,\n",
        "                                     dataset_path=valid_dataset_path ,\n",
        "                                     system_message= system_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U6dbqF4jlGp"
      },
      "source": [
        "## Start Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaBfZQySAWOw"
      },
      "outputs": [],
      "source": [
        "fine_tuner.train(train_dataset_mapped=train_dataset_mapped,valid_dataset_mapped=valid_dataset_mapped , output_dir=\"LLAMA2_chat\" ,num_train_epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skJDiPZRjvwg"
      },
      "source": [
        "## Create Inference Pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cp9BUa_OC8OG"
      },
      "outputs": [],
      "source": [
        "from qlora_tunner.qlora_inference import LanguageModelInference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9FPKYn-hDDo_"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "LoraConfig_folder = \"\" # LLAMA2_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B6ZGgUXDTZL"
      },
      "outputs": [],
      "source": [
        "inference = LanguageModelInference(model_name)\n",
        "pipeline = inference.inf_pipeline(max_length=2048 , LoraConfig_file=LoraConfig_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8_-vYoj2Qn"
      },
      "source": [
        "## Run the Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mhDUbsmYD1jo"
      },
      "outputs": [],
      "source": [
        "system_message = \" \"   # Example :  \"You are a helpful AI bot\"\n",
        "input_text = \" \"       # Example : you can Ask any Question related to your task \n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n {input_text}. [/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA8R7P93D4hw"
      },
      "outputs": [],
      "source": [
        "result = pipeline(prompt)\n",
        "print(result[0]['generated_text'].replace(prompt, ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99xTddj8j6mm"
      },
      "source": [
        "## With Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3hqZZzTbhhw",
        "outputId": "e4127141-eaf8-4dfe-ec9b-86d2d3173f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip -q install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8TT_OCcybhVq"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "local_llm = HuggingFacePipeline(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JFgdoMdKcCgt"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Htqs2bnBcE41"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "chat = ConversationChain(\n",
        "    llm=local_llm,\n",
        "    verbose=False ,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frVU59rTcIB0"
      },
      "outputs": [],
      "source": [
        "chat.prompt.template = \\\n",
        "\"\"\"\n",
        "### HUMAN:\n",
        "You are a helpful AI bot.\n",
        "Previous Conversation :\n",
        "{history}\n",
        "\n",
        "Current conversation:\n",
        "### HUMAN: {input}\n",
        "### RESPONSE:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUy7UrnLcKdA"
      },
      "outputs": [],
      "source": [
        "while 1 :\n",
        "  input_text = input(\">>\")\n",
        "  print(chat.predict(input=str(input_text)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4seCd30kfXZq",
        "JS8_-vYoj2Qn",
        "99xTddj8j6mm"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
